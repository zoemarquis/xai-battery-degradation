{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,KFold \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error, root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taille_fenetre_to_run = \"courte\"\n",
    "assert taille_fenetre_to_run in [\"courte\",\"moyenne\",\"longue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if taille_fenetre_to_run == \"courte\":\n",
    "    data = pickle.load(open(\"Data/donnees_courte.pkl\", \"rb\"))\n",
    "elif taille_fenetre_to_run == \"moyenne\":\n",
    "    data = pickle.load(open(\"Data/donnees_moyenne.pkl\", \"rb\"))\n",
    "else:\n",
    "    data = pickle.load(open(\"Data/donnees_longue.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_np_label', 'X_np_binary', 'y_np', 'X_df_label', 'X_df_binary', 'y_df'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np_label = data[\"X_np_label\"]\n",
    "y_np = data[\"y_np\"]\n",
    "\n",
    "X_df_label = data[\"X_df_label\"]\n",
    "y_df = data[\"y_df\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'cycle_life',\n",
       " 1: 'C1',\n",
       " 2: 'C2',\n",
       " 3: 'Q1',\n",
       " 4: 'IR',\n",
       " 5: 'QC',\n",
       " 6: 'Tavg',\n",
       " 7: 'Tmin',\n",
       " 8: 'Tmax',\n",
       " 9: 'chargetime',\n",
       " 10: 'pulse_width',\n",
       " 11: 'cycle_to_x%',\n",
       " 12: 'cutoff_currents',\n",
       " 13: 'charging_rest',\n",
       " 14: 'after_discharging_rest',\n",
       " 15: 'before_discharging_rest',\n",
       " 16: 'IR_rest',\n",
       " 17: 'barcode_label',\n",
       " 18: 'cycle_normalized'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_label = pickle.load(open(\"Data/features_label.pkl\", \"rb\"))\n",
    "features_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que toutes les données de X_np_label sont entre 0 et 1\n",
    "for i in range(X_np_label.shape[0]):\n",
    "    for j in range(X_np_label.shape[1]):\n",
    "        for k in range(X_np_label.shape[2]):\n",
    "            if k != 17:\n",
    "                assert X_np_label[i, j, k] >= 0 and X_np_label[i, j, k] <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4711, 50, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape les données 3D pour les adapter à un random forest\n",
    "X_reshape = X_np_label.reshape(X_np_label.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1/5\n",
      "Running fold 2/5\n",
      "Running fold 3/5\n",
      "Running fold 4/5\n",
      "Running fold 5/5\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, random_state=42, shuffle=True) \n",
    "# LSTM : on pourrait tester stratified (surtout dans lstm où performance dépend vraiment du split) pour l'instante)\n",
    "\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_reshape)):\n",
    "    print(f\"Running fold {fold+1}/{n_splits}\")\n",
    "    X_train, X_test = X_reshape[train_index], X_reshape[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "    \n",
    "    rf.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = f'Models/RandomForest_{taille_fenetre_to_run}_fold_{fold}.sav'\n",
    "    pickle.dump(rf, open(filename, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE: 0.24853822811764337\n",
      "Mean MAE: 0.278908448004453\n",
      "Mean MAPE: 0.0029990265106541896\n",
      "Mean RMSE: 0.4864666969305317\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean MSE: {np.mean(mse_scores)}\")\n",
    "print(f\"Mean MAE: {np.mean(mae_scores)}\")\n",
    "print(f\"Mean MAPE: {np.mean(mape_scores)}\")\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE scores: [np.float64(0.16302421755448757), np.float64(0.48777926714447445), np.float64(0.2246323130303261), np.float64(0.19978039537178555), np.float64(0.16747494748714314)]\n",
      "MAE scores: [np.float64(0.2561720295922761), np.float64(0.2998241658632217), np.float64(0.29764363279876144), np.float64(0.2784366329249412), np.float64(0.2624657788430647)]\n",
      "MAPE scores: [np.float64(0.002744264179255248), np.float64(0.0032424917410425157), np.float64(0.003207671722311425), np.float64(0.00298544455762819), np.float64(0.0028152603530335702)]\n",
      "RMSE scores: [np.float64(0.40376257572302016), np.float64(0.6984119609116631), np.float64(0.4739539144582795), np.float64(0.44696800262634634), np.float64(0.40923703093334934)]\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE scores: {mse_scores}')\n",
    "print(f'MAE scores: {mae_scores}')\n",
    "print(f'MAPE scores: {mape_scores}')\n",
    "print(f'RMSE scores: {rmse_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : mettre en place dans autre fichier gridsearch \n",
    "# taille de fenetre (mais on pourrait aussi tester avec \n",
    "# stride différent... à voir à quel point c'est long )\n",
    "# n_estimators\n",
    "# max_depth\n",
    "# min_samples_split\n",
    "# min_samples_leaf\n",
    "# max_features\n",
    "# bootstrap\n",
    "# criterion\n",
    "# etc\n",
    "\n",
    "# enfaite si on fait le gridsearch sur taille de fenetre d'abord en random forest\n",
    "# ensuite on peut dire qu'on utilise meme taille de fenetre / stride pour rnn et linéaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : pas prendre un modèle de la validation croisée mais \n",
    "# directement un modèle (sinon biais ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : lstm : RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
